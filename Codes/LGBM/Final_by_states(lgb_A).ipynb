{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_by_states(lgb_A).ipynb","provenance":[],"collapsed_sections":["jDeLvQS7IBBF"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"wJE3CqCPlekG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1594736732060,"user_tz":-480,"elapsed":24429,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"710a666a-29d9-4b60-8251-1194edd81cfd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DNIG9EtMl4ON","colab_type":"text"},"source":["## 导入工具包及设置路径"]},{"cell_type":"code","metadata":{"id":"qZ_FWMeSlk4N","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594736733684,"user_tz":-480,"elapsed":866,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}}},"source":["import numpy as np\n","import pandas as pd\n","import os, sys, gc, time, warnings, pickle, psutil, random\n","from multiprocessing import Pool   \n","\n","warnings.filterwarnings('ignore')\n","fea_pkl_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/pkl/fea_pkl/'\n","model_pkl_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/pkl/model_pkl/'\n","datasets_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/datasets/'\n","fea_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/Features/'\n","pred_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/Predict/'\n","sub_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/Predict/sub/'\n","\n","BASE = fea_pkl_path + 'grid_part_1.pkl'\n","PRICE = fea_pkl_path + 'grid_part_2.pkl'\n","CALENDAR = fea_pkl_path + 'grid_part_3.pkl'\n","LAGS = fea_pkl_path + 'grid_part_4.pkl'\n","MEAN_ENC = fea_pkl_path + 'grid_part_5.pkl'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8anlfhfvl0v3","colab_type":"text"},"source":["## 基本函数"]},{"cell_type":"code","metadata":{"id":"M3v6yBvitYGM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594736736630,"user_tz":-480,"elapsed":938,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}}},"source":["## 设定随机种子\n","def seed_everything(seed=0):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","\n","## 按照商店读取数据\n","def get_data_by_state(state):\n","  df = pd.concat([pd.read_pickle(BASE), # 9\n","            pd.read_pickle(PRICE).iloc[:,2:], # 11\n","            pd.read_pickle(CALENDAR).iloc[:,2:], # 15 \n","            pd.read_pickle(LAGS).iloc[:,3:], # 37\n","            pd.read_pickle(MEAN_ENC)[mean_features]], # 14(按商店只有6)  \n","            axis=1) \n","  # df：(47735397, 86)由于行数相同，可以横向拼接\n","  df = df[df['state_id'] == state] \n","  features = [col for col in list(df) if col not in remove_features] # 86 - 6(BASE) - 1(Holiday) = 79\n","  df = df[['id', 'd', TARGET] + features] # 79 + 3 = 82 相当于去掉了 'state_id', 'store_id', 'release', 'Holiday'(在CALENDAR里面)\n","  # 选训练开始的天数\n","  df = df[df['d'] >= START_TRAIN].reset_index(drop=True)\n","  return df, features\n","\n","\n","## 读取测试数据\n","def get_base_test():\n","  base_test = pd.DataFrame()\n","  # new_col = ['event_name_1_win', 'event_name_2_win'] \n","  if USE_AUX:\n","    model_path = model_pkl_path\n","  else:\n","    model_path=''\n","    \n","  for state_id in STATE_IDS: \n","    temp_df = pd.read_pickle(model_path + 'test_{}.pkl'.format(state_id)) # 不含lag + rolling特征\n","    temp_df['state_id'] = state_id\n","    base_test = pd.concat([base_test, temp_df]).reset_index(drop=True) # 纵向拼接\n","  \n","  # base_test[new_col] = base_test[new_col].astype('category') ### 注意：这个地方必须要把这三列转化为 category 类型，否则没办法预测\n","  return base_test\n","## 制作lag特征\n","def make_lag(LAG_DAY):\n","  lag_df = base_test[['id','d',TARGET]]\n","  col_name = 'sales_lag_' + str(LAG_DAY)\n","  lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n","  return lag_df[[col_name]]\n","## 递归特征\n","def make_lag_roll(LAG_DAY):\n","  shift_day = LAG_DAY[0]\n","  roll_wind = LAG_DAY[1]\n","  lag_df = base_test[['id','d',TARGET]]\n","  col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n","  lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n","  return lag_df[[col_name]]\n","## 多线程执行，用于测试集融合\n","def df_parallelize_run(func, t_split):\n","  num_cores = np.min([N_CORES,len(t_split)])\n","  pool = Pool(num_cores)\n","  df = pd.concat(pool.map(func, t_split), axis=1)\n","  pool.close()\n","  pool.join()\n","  return df\n","\n","# lag + rolling\n","SHIFT_DAY = 28\n","N_LAGS = 15\n","LAGS_SPLIT = [col for col in range(SHIFT_DAY, SHIFT_DAY + N_LAGS)]\n","ROLS_SPLIT = []\n","for i in [1,7,14]:\n","  for j in [7,14,30,60]:\n","    ROLS_SPLIT.append([i,j])\n","   \n","USE_AUX = False\n","if USE_AUX:\n","  lgb_params['n_estimators'] = 2\n","\n","VER = 1 # 设置模型的版本\n","SEED = 42 \n","seed_everything(SEED) # 消除随机性\n","N_CORES = psutil.cpu_count() # 可使用的CPU内核\n","\n","TARGET = 'sales' # Label\n","START_TRAIN = 0 # 能跳过一些行(Nans/faster training)\n","P_HORIZON = 28 # 预测范围\n","USE_AUX = True # 使用预训练好的模型             \n","\n","remove_features = ['id', 'state_id', 'store_id', 'release', 'd', 'Holiday', TARGET]                    \n","mean_features   = [ 'enc_cat_id_mean', 'enc_cat_id_std', \n","             'enc_store_id_mean', 'enc_store_id_std', \n","             'enc_dept_id_mean', 'enc_dept_id_std', \n","             'enc_item_id_mean', 'enc_item_id_std', \n","             'enc_item_id_store_id_mean', 'enc_item_id_store_id_std',\n","             'enc_store_id_cat_id_std', 'enc_state_id_cat_id_std',\n","             'enc_store_id_dept_id_mean', 'enc_store_id_dept_id_std' ]  # 14\n","# 按州分别训练，每个州可以按商店、类别、部门、商品的销量聚合取mean\\std，可以选这14个特征"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mk_wZzq9snqp","colab_type":"text"},"source":["## lgbm模型参数"]},{"cell_type":"code","metadata":{"id":"TLHbJ6BosmcK","colab_type":"code","colab":{}},"source":["import lightgbm as lgb\n","lgb_WI_params = {\n","            'boosting_type': 'gbdt',\n","            'objective': 'tweedie',\n","            'tweedie_variance_power': 1.1,\n","            'metric': 'rmse',\n","            'subsample': 0.5,\n","            'subsample_freq': 1,\n","            'learning_rate': 0.03,\n","            'num_leaves': 2**11-1,\n","            'min_data_in_leaf': 2**12-1,\n","            'feature_fraction': 0.5,\n","            'max_bin': 100,\n","            'n_estimators': 1400,\n","            'boost_from_average': False,\n","            'verbose': -1,\n","            'seed': 42,\n","          } \n","\n","lgb_CA_params =  {\n","            'boosting_type': 'gbdt',\n","            'objective': 'tweedie',\n","            'metric': 'rmse',\n","            'bagging_freq': 0,\n","            'feature_fraction': 0.9329928507911868,\n","            'lambda_l2': 0.021982796763644744,\n","            'learning_rate': 0.04723157294394453,\n","            'max_bin': 67,\n","            'min_data_in_leaf': 11801,\n","            'n_estimators': 1512,\n","            'num_leaves': 25631,\n","            'sub_feature': 0.6053325030777185,\n","            'sub_row': 0.6184421797679618,\n","            'subsample': 0.5951931300022044,\n","            'subsample_freq': 3,\n","            'tweedie_variance_power': 1.1833186379351004,\n","            'seed': 42,\n","            'boost_from_average': False,\n","          }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vex2uvzs6TKm","colab_type":"text"},"source":["## 每个州单独训练(按州TX聚合效果差，只聚合CA和WI)"]},{"cell_type":"code","metadata":{"id":"VyhF8v21tjdy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594736756047,"user_tz":-480,"elapsed":804,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}}},"source":["STATE_IDS = ['CA','WI'] \n","END_TRAIN = 1913"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"NI1vHx6nOEhE","colab_type":"code","colab":{}},"source":["# features_columns = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_store_id_mean', 'enc_store_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'enc_item_id_store_id_mean', 'enc_item_id_store_id_std', 'enc_store_id_cat_id_std', 'enc_state_id_cat_id_std', 'enc_store_id_dept_id_mean', 'enc_store_id_dept_id_std']\n","# 79"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WY8SZerBtlJ7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":575},"executionInfo":{"status":"ok","timestamp":1594379780430,"user_tz":-480,"elapsed":143755,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"c27a5347-3997-47d1-f1e1-464f087c01a7"},"source":["for state_id in STATE_IDS:\n","  print('Train', state_id)\n","  grid_df, features_columns = get_data_by_state(state_id) # grid_df：(14599119, 82)  features_columns：79(相比于grid_df少了id、d、sales)\n","  ########################## 添加节假日特性 #############################\n","  calendar_win = pd.read_csv(datasets_path + 'calendar.csv')[['event_name_2','event_name_1','d']]\n","  # 添加特征\n","  event = calendar_win[['event_name_2','event_name_1']].fillna('') \n","  # 整体上移一天\n","  event_before1 = pd.DataFrame()\n","  event_up_one = event.shift(-1).fillna('') \n","  event_before1['event_name_2'] = event_up_one['event_name_2'].apply(lambda x: x + '_before1' if x != '' else x)\n","  event_before1['event_name_1'] = event_up_one['event_name_1'].apply(lambda x: x + '_before1' if x != '' else x)\n","  # 整体上移两天\n","  event_before2 = pd.DataFrame()\n","  event_up_two = event.shift(-2).fillna('')   \n","  event_before2['event_name_2'] = event_up_two['event_name_2'].apply(lambda x: x + '_before2' if x != '' else x)\n","  event_before2['event_name_1'] = event_up_two['event_name_1'].apply(lambda x: x + '_before2' if x != '' else x)\n","  event_win = event_before1 + event_before2\n","\n","  calendar_win.drop(columns=['event_name_1','event_name_2'],inplace=True)\n","\n","  # 转换数据类型\n","  calendar_win['event_name_1_win'] = event_win['event_name_1'].astype('category')\n","  calendar_win['event_name_2_win'] = event_win['event_name_2'].astype('category')\n","  calendar_win['d'] = calendar_win['d'].apply(lambda x: x.split('_',2)[1]).astype(np.int16)\n","\n","  # 小融入大 merge\n","  grid_df = grid_df.merge(calendar_win, on='d', how='left')\n","  features_columns = features_columns + ['event_name_1_win','event_name_2_win']\n","\n","  train_mask = grid_df['d'] <= END_TRAIN - P_HORIZON # 1<= train <=1941-28 \n","  valid_mask = (grid_df['d'] > END_TRAIN - P_HORIZON) & (grid_df['d'] <= END_TRAIN) # 1941-28< valid <=1941\n","  preds_mask = grid_df['d'] > END_TRAIN - 100 # 1941-100< test <=1969 \n","\n","  \n","  train_data = lgb.Dataset(grid_df[train_mask][features_columns], label=grid_df[train_mask][TARGET])\n","  # train_data.save_binary(save_dir + 'train_data.bin')\n","  valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], label=grid_df[valid_mask][TARGET])\n","  # valid_data.save_binary(save_dir + 'valid_data.bin')\n","\n","  # 为了之后的预测，需要保留部分数据集，因为迭代特征需要用到前面天的\"Sales\"\n","  grid_df = grid_df[preds_mask].reset_index(drop=True) \n","  # 原来已经做好的\"lag + rolling\"特征需要去掉，然后在新的一部分数据集重新生成\n","  keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n","  # 细节：在测试集中去掉了递归特征\n","  grid_df = grid_df[keep_cols] \n","  grid_df.to_pickle(model_pkl_path + 'test_{}.pkl'.format(state_id))\n","  del grid_df\n","  \n","  seed_everything(SEED)\n","  if state_id == 'CA':\n","    lgb_params = lgb_CA_params\n","  elif state_id == 'WI':\n","    lgb_params = lgb_WI_params\n","\n","  estimator = lgb.train(lgb_params, train_data, num_boost_round = 10000, valid_sets = [valid_data], verbose_eval = 100, early_stopping_rounds = 100)\n","  # valid_sets = [train_data, valid_data] 是用来显示它俩训练得到的rmse，没有它，就不显示，而verbose_eval = 100表示迭代100次打印一次它俩的rmse值\n","  # early_stopping_rounds 防止过拟合(随着轮数的增加，验证误差不降反升) \n","  # 教训：如果设置提前停止，则需要在 valid_sets 中至少有一个集合，如果有多个，它们都会被使用，一般只看验证集的误差\n","  # 如果在训练过程中启用了提前停止, 可以用 bst.best_iteration 从最佳迭代中获得预测结果 即：num_iteration = estimator.best_iteration\n","  model_name = model_pkl_path + 'lgb_model_{}_v{}.bin'.format(state_id, str(VER))\n","  pickle.dump(estimator, open(model_name, 'wb'))\n","  del train_data, valid_data, estimator\n","  gc.collect()\n","\n","  MODEL_FEATURES = features_columns"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train CA\n","Training until validation scores don't improve for 100 rounds.\n","[100]\tvalid_0's rmse: 1.98929\n","[200]\tvalid_0's rmse: 1.97147\n","[300]\tvalid_0's rmse: 1.9628\n","[400]\tvalid_0's rmse: 1.95648\n","[500]\tvalid_0's rmse: 1.95299\n","[600]\tvalid_0's rmse: 1.95188\n","[700]\tvalid_0's rmse: 1.95101\n","[800]\tvalid_0's rmse: 1.95024\n","[900]\tvalid_0's rmse: 1.9502\n","Early stopping, best iteration is:\n","[847]\tvalid_0's rmse: 1.95014\n","Train WI\n","Training until validation scores don't improve for 100 rounds.\n","[100]\tvalid_0's rmse: 2.11387\n","[200]\tvalid_0's rmse: 2.06747\n","[300]\tvalid_0's rmse: 2.05855\n","[400]\tvalid_0's rmse: 2.05335\n","[500]\tvalid_0's rmse: 2.05126\n","[600]\tvalid_0's rmse: 2.04895\n","[700]\tvalid_0's rmse: 2.0472\n","[800]\tvalid_0's rmse: 2.04642\n","[900]\tvalid_0's rmse: 2.04606\n","[1000]\tvalid_0's rmse: 2.04544\n","[1100]\tvalid_0's rmse: 2.04388\n","[1200]\tvalid_0's rmse: 2.04297\n","[1300]\tvalid_0's rmse: 2.04303\n","[1400]\tvalid_0's rmse: 2.04222\n","Did not meet early stopping. Best iteration is:\n","[1389]\tvalid_0's rmse: 2.04207\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u-FwIImBiVD4","colab_type":"text"},"source":["## 预测"]},{"cell_type":"code","metadata":{"id":"4ocYPQzUiIi9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594736744923,"user_tz":-480,"elapsed":810,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}}},"source":["# 训练特征 79 + 2 = 81 \n","MODEL_FEATURES = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_store_id_mean', 'enc_store_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'enc_item_id_store_id_mean', 'enc_item_id_store_id_std', 'enc_store_id_cat_id_std', 'enc_state_id_cat_id_std', 'enc_store_id_dept_id_mean', 'enc_store_id_dept_id_std', 'event_name_1_win', 'event_name_2_win']"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGQPtpnQtsBq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"status":"error","timestamp":1594736764115,"user_tz":-480,"elapsed":1941,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"27276955-2031-401d-adb0-d6ae2257e7c9"},"source":["USE_AUX = True\n","END_TRAIN = 1913\n","all_preds = pd.DataFrame()\n","base_test = get_base_test() # [2731904 rows x 73 columns]\n","main_time = time.time()\n","temp_fe = ['rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14',\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n","       \n","# tem = base_test[base_test['id'] == 'HOUSEHOLD_2_490_WI_3_evaluation'][['sales']]\n","# tem.to_csv(pred_path + 'ori.csv'.format(PREDICT_DAY, state_id))\n","\n","for PREDICT_DAY in range(1,29):    \n","    print('Predict | Day:', PREDICT_DAY)\n","    start_time = time.time()\n","    # 加上测试集\"lag+rolling\"特征(12个)\n","    grid_df = base_test.copy() # 临时的一个df\n","    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n","    # print(grid_df.columns)\n","###### 这个函数非常重要，预测完一天之后，base_test的销量就会加上前面天数预测的结果，俗称：\"迭代预测\" #######\n","###### 因为make_lag_roll里面按照base_test的\"sales\"进行lag+rolling，而base_test的\"sales\"又是随着预测不断变动的 #######\n","\n","    for state_id in STATE_IDS:\n","      model_bin = 'lgb_model_{}_v{}.bin'.format(state_id, str(VER))\n","      if USE_AUX:\n","        model_path = model_pkl_path + model_bin\n","      # 加载训练好的模型\n","      estimator = pickle.load(open(model_path, 'rb'))\n","      \n","      day_mask = base_test['d'] == END_TRAIN + PREDICT_DAY # 某一天\n","      store_mask = base_test['state_id'] == state_id # 某一个州\n","      mask = (day_mask) & (store_mask) # 这个州的某一天用来预测\n","      base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES], num_iteration = estimator.best_iteration) \n","      # 比如预测得到的1942这一天的销量，把它填充到base_test的\"sales\"对应1942这一天\"NAN\"值\n","      # 所以，base_test的\"sales\"一直在变，所以下面预测1943这一天的销量，会用到前面1942这一天的预测值\n","\n","    temp_df = base_test[day_mask][['id',TARGET]]\n","    temp_df.columns = ['id', 'F{}'.format(str(PREDICT_DAY))]\n","    if 'id' in list(all_preds):\n","        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n","    else:\n","        all_preds = temp_df.copy()\n","        \n","    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n","            ' %0.2f min total |' % ((time.time() - main_time) / 60),\n","            ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n","    del temp_df\n","    \n","all_preds = all_preds.reset_index(drop=True)\n","all_preds.to_csv(pred_path + 'per_state(WI+CA)_pred_A.csv')\n","\n","# 用于生成提交预测的文件\n","submission = pd.read_csv(datasets_path + 'sample_submission.csv')[['id']] # [[]] 返回DataFrame [] 返回Series\n","submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n","submission.to_csv(sub_path + 'submission_by_states(lgb)_A.csv', index=False) "],"execution_count":7,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-41951f918fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEND_TRAIN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1913\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbase_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_base_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [2731904 rows x 73 columns]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m temp_fe = ['rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14',\n","\u001b[0;32m<ipython-input-3-4242e6c6c630>\u001b[0m in \u001b[0;36mget_base_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstate_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTATE_IDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'test_{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 不含lag + rolling特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mbase_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 纵向拼接\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/pkl/model_pkl/test_CA.pkl'"]}]},{"cell_type":"markdown","metadata":{"id":"jDeLvQS7IBBF","colab_type":"text"},"source":["## 某个州进行贝叶斯调参"]},{"cell_type":"code","metadata":{"id":"GgQkBLiQIAgd","colab_type":"code","colab":{}},"source":["STATE_IDS = 'TX'\n","grid_df, features_columns = get_data_by_state(STATE_IDS)\n","day = 1941\n","grid_df1 = grid_df[grid_df['d'] <= day] \n","del grid_df\n","\n","!pip install bayesian-optimization\n","\n","import lightgbm as lgb\n","from bayes_opt import BayesianOptimization\n","# 定义RMSE\n","def rmse(y, y_pred):\n","  return np.sqrt(np.mean(np.square(y - y_pred)))\n","\n","df = grid_df1\n","fe = features_columns\n","# 1000到1941-28-28 作为训练集\n","tr_x, tr_y = df[(df['d'] >= 1000) & (df['d'] <= (day-28-28))][fe], df[(df['d'] >= 1000) & (df['d'] <= (day-28-28))]['sales'] \n","# 1941-28-28到1941-28 作为验证集\n","vl_x, vl_y = df[(df['d'] > (day-28-28)) & (df['d'] <= (day-28))][fe], df[(df['d'] > (day-28-28)) & (df['d'] <= (day-28))]['sales']\n","\n","train_data = lgb.Dataset(tr_x, label = tr_y)\n","valid_data = lgb.Dataset(vl_x, label = vl_y)\n","\n","# 1914到1941 作为测试集，用训练的模型预测它的\"sales\"，再与真实的\"sales\"计算RMSE\n","test_df = df[df['d'] > day-28].reset_index(drop=True) \n","\n","# 定义黑盒函数\n","def lgb_cv(tweedie_variance_power, subsample, subsample_freq, learning_rate, num_leaves, min_data_in_leaf,\n","      feature_fraction, max_bin, n_estimators, lambda_l2, sub_row, sub_feature, bagging_freq):\n","\n","\n","    lgb_params = {\n","                    'boosting_type': 'gbdt',\n","                    'objective': 'tweedie',\n","                    'tweedie_variance_power': tweedie_variance_power,\n","                    'metric': 'rmse',\n","                    'subsample': subsample,\n","                    'subsample_freq': int(subsample_freq),\n","                    'learning_rate': learning_rate,\n","                    'num_leaves': int(num_leaves),\n","                    'min_data_in_leaf': int(min_data_in_leaf),\n","                    'feature_fraction': feature_fraction,\n","                    'max_bin': int(max_bin),\n","                    'n_estimators': int(n_estimators),\n","                    'boost_from_average': False,\n","                    'seed': 42,  \n","                    'lambda_l2':lambda_l2,\n","                    'sub_row':sub_row,  \n","                    'sub_feature':sub_feature,\n","                    'bagging_freq':int(bagging_freq)          \n","                }\n","    stimator = lgb.train(lgb_params, train_data, num_boost_round = 10000, valid_sets = [valid_data], verbose_eval = 100, early_stopping_rounds = 100) \n","    # valid_sets = [train_data, valid_data] 这个是用来显示training's rmse和valid_1's rmse，这里不用显示，就不要了\n","    # verbose_eval : 迭代多少次打印\n","    test_df['preds'] = stimator.predict(test_df[fe], num_iteration = stimator.best_iteration)\n","    base_score = rmse(test_df['sales'], test_df['preds'])\n","\n","    return -base_score\n","\n","# 给定超参数搜索空间\n","opt = BayesianOptimization(\n","                lgb_cv,\n","                {\n","                    'tweedie_variance_power': (1, 1.3),\n","                    'subsample': (0.5, 1.0),\n","                    'subsample_freq': (0, 5),\n","                    'learning_rate': (0, 1),\n","                    'num_leaves':(2**10-1,2**15-1),\n","                    'min_data_in_leaf':(2**10-1,2**15-1),\n","                    'feature_fraction':(0.4,1),\n","                    'max_bin':(80,150),\n","                    'n_estimators':(1000,1700),\n","                    'lambda_l2':(0,0.2),\n","                    'sub_row':(0.6,1),\n","                    'sub_feature':(0.6,1.0),\n","                    'bagging_freq':(0,5),\n","                }\n","              )\n","opt.maximize(n_iter = 20) # 最大化黑盒函数\n","rf_bo.max # 返回黑盒函数值最大的超参数"],"execution_count":null,"outputs":[]}]}