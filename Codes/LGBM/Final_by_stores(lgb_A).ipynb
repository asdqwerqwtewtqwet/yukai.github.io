{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_by_stores(lgb_A).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"z_5SVT6RvYYq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1594300140836,"user_tz":-480,"elapsed":18780,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"9ab54520-20f8-4f82-8eb4-6a1066550a90"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jiFkJMb3Ey0F","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594300143575,"user_tz":-480,"elapsed":1759,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}}},"source":["import numpy as np\n","import pandas as pd\n","import os, sys, gc, time, warnings, pickle, psutil, random\n","import lightgbm as lgb\n","from multiprocessing import Pool        \n","warnings.filterwarnings('ignore')\n","\n","fea_pkl_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/pkl/fea_pkl/'\n","model_pkl_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/pkl/model_pkl/'\n","datasets_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/datasets/'\n","fea_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/Features/'\n","pred_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/Predict/'\n","sub_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/Predict/sub/'\n","\n","BASE = fea_pkl_path + 'grid_part_1.pkl'\n","PRICE = fea_pkl_path + 'grid_part_2.pkl'\n","CALENDAR = fea_pkl_path + 'grid_part_3.pkl'\n","LAGS = fea_pkl_path + 'grid_part_4.pkl'\n","MEAN_ENC = fea_pkl_path + 'grid_part_5.pkl'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhPOT38tOl_q","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594300146364,"user_tz":-480,"elapsed":810,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}}},"source":["## 设定随机种子\n","def seed_everything(seed=0):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","\n","## 按照商店读取数据\n","def get_data_by_store(store):\n","    # 读取和连接基本特征\n","    df = pd.concat([pd.read_pickle(BASE), # 9\n","             pd.read_pickle(PRICE).iloc[:,2:], # 11\n","             pd.read_pickle(CALENDAR).iloc[:,2:], # 15 \n","             pd.read_pickle(LAGS).iloc[:,3:], # 37\n","             pd.read_pickle(MEAN_ENC)[mean_features]], # 6  \n","             axis=1) \n","    # df：(47735397, 78)由于行数相同，可以横向拼接\n","    df = df[df['store_id'] == store] # df['store_id'] == store 返回布尔Series，df[df['store_id'] == store] 然后取True的行构成的dataframe\n","    features = [col for col in list(df) if col not in remove_features] # 78 - 7 = 71\n","    df = df[['id', 'd', TARGET] + features] # 71 + 3 = 74 相当于去掉了 'state_id', 'store_id', 'release', 'Holiday'(在CALENDAR里面)\n","    # 选训练开始的天数\n","    df = df[df['d'] >= START_TRAIN].reset_index(drop=True) # drop = True去掉生成的原索引列\n","    return df, features\n","\n","\n","## 读取测试数据\n","def get_base_test():\n","  base_test = pd.DataFrame()\n","  new_col = ['event_name_1_win', 'event_name_2_win', 'event_name_1_win_1'] # 因为 store_id 在训练模型的时候没用到这个特征，所以不用管\n","  if USE_AUX:\n","    model_path = model_pkl_path\n","  else:\n","    model_path=''\n","\n","  for store_id in STORES_IDS: \n","    temp_df = pd.read_pickle(model_path + 'test_{}_A.pkl'.format(store_id)) # 不含lag + rolling特征\n","    temp_df['store_id'] = store_id\n","    base_test = pd.concat([base_test, temp_df]).reset_index(drop=True) # 纵向拼接\n","  \n","  base_test[new_col] = base_test[new_col].astype('category') ### 注意：这个地方必须要把这三列转化为 category 类型，否则没办法预测\n","  return base_test\n","## 制作lag特征\n","def make_lag(LAG_DAY):\n","  lag_df = base_test[['id','d',TARGET]]\n","  col_name = 'sales_lag_' + str(LAG_DAY)\n","  lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n","  return lag_df[[col_name]]\n","## 递归特征\n","def make_lag_roll(LAG_DAY):\n","  shift_day = LAG_DAY[0]\n","  roll_wind = LAG_DAY[1]\n","  lag_df = base_test[['id','d',TARGET]]\n","  col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n","  lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n","  return lag_df[[col_name]]\n","## 多线程执行，用于测试集融合\n","def df_parallelize_run(func, t_split):\n","  num_cores = np.min([N_CORES,len(t_split)])\n","  pool = Pool(num_cores)\n","  df = pd.concat(pool.map(func, t_split), axis=1)\n","  pool.close()\n","  pool.join()\n","  return df\n","\n","# lag + rolling\n","SHIFT_DAY = 28\n","N_LAGS = 15\n","LAGS_SPLIT = [col for col in range(SHIFT_DAY, SHIFT_DAY + N_LAGS)]\n","ROLS_SPLIT = []\n","for i in [1,7,14]:\n","  for j in [7,14,30,60]:\n","    ROLS_SPLIT.append([i,j])\n","\n","# grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n","\n","\n","VER = 1 # 设置模型的版本\n","SEED = 42 \n","seed_everything(SEED) # 消除随机性\n","N_CORES = psutil.cpu_count() # 可使用的CPU内核\n","\n","TARGET = 'sales' # Label\n","START_TRAIN = 0 # 能跳过一些行(Nans/faster training)\n","P_HORIZON = 28 # 预测范围\n","USE_AUX = True # 使用预训练好的模型\n","\n","remove_features = ['id', 'state_id', 'store_id', 'release', 'Holiday', 'd', TARGET]          \n","mean_features = ['enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std'] \n","# 按商店分别训练，每个商店可以只能按类别、部门、商品的销量聚合取mean\\std，故只选这6个特征"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_R3Odek2Vl1","colab_type":"text"},"source":["## 每个商店的lgbm参数"]},{"cell_type":"code","metadata":{"id":"TbHapyQzzrcE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594300150989,"user_tz":-480,"elapsed":848,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}}},"source":["## CA1的参数\n","lgb_CA1_params =  {\n","              'boosting_type': 'gbdt',\n","              'objective': 'tweedie',\n","              'metric': 'rmse',\n","              'bagging_freq': 0,              ## 又名 subsample_freq\n","              'feature_fraction': 0.9329928507911868,   ## 又名sub_feature, colsample_bytree\n","              'lambda_l2': 0.021982796763644744,\n","              'learning_rate': 0.04723157294394453,\n","              'max_bin': 67,\n","              'min_data_in_leaf': 11801,\n","              'n_estimators': 1512,\n","              'num_leaves': 25631,            # 换算成2**depth\n","              'sub_feature': 0.6053325030777185,      ## 又名feature_fraction, colsample_bytree\n","              'subsample': 0.6184421797679618,\n","              'subsample_freq': 2,\n","              'tweedie_variance_power': 1.1833186379351004,\n","              'seed': 42,\n","              'boost_from_average': False\n","           }\n","## CA3的参数\n","lgb_CA3_params =  {\n","            'boosting_type': 'gbdt',\n","            'objective': 'tweedie',\n","            'metric': 'rmse',\n","            'bagging_freq': 0,\n","            'feature_fraction': 0.8050328925461323,\n","            'lambda_l2': 0.05756643694569541,\n","            'learning_rate': 0.017804685422408588,\n","            'max_bin': 100,\n","            'min_data_in_leaf': 6227,\n","            'n_estimators': 1542,\n","            'num_leaves': 3479,\n","            'sub_feature': 0.9535898390938982,\n","            'sub_row': 0.8458711078426921,\n","            'subsample': 0.5687395598191918,\n","            'subsample_freq': 2,\n","            'tweedie_variance_power': 1.3810263355485752,\n","            'seed': 42,\n","            'boost_from_average': False,\n","           }\n","## 其他商店的参数\n","lgb_others_params = {\n","              'boosting_type': 'gbdt',\n","              'objective': 'tweedie',\n","              'tweedie_variance_power': 1.1,\n","              'metric': 'rmse',\n","              'subsample': 0.5,\n","              'subsample_freq': 1,\n","              'learning_rate': 0.03,\n","              'num_leaves': 2**11-1,\n","              'min_data_in_leaf': 2**12-1,\n","              'feature_fraction': 0.5,\n","              'max_bin': 100,\n","              'n_estimators': 1400,\n","              'boost_from_average': False,\n","              'verbose': -1,\n","              'seed': 42 \n","           }"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52JVXYxX2heR","colab_type":"text"},"source":["## 每个商店单独训练模型"]},{"cell_type":"code","metadata":{"id":"C84oJ59GPJ25","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594303743611,"user_tz":-480,"elapsed":3590502,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"8ad63425-baa8-492c-f52b-d6bca0f9a1c6"},"source":["STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']\n","END_TRAIN = 1913\n","############################################# Train Models ###############################################\n","for store_id in STORES_IDS: # 每个商店单独训练\n","    print('Train', store_id)\n","    grid_df, features_columns = get_data_by_store(store_id) # grid_df：74 features_columns：71\n","\n","    ########################## 添加节假日特性 #############################\n","    ## CA1、TX2、TX3的特征的节假日效应类似，用同一个文件\n","    if (store_id=='CA_1') or (store_id=='TX_2') or (store_id=='TX_3'):\n","      calendar_win = pd.read_csv(fea_path + 'CA1_TX2_TX3_holidays.csv')\n","      calendar_win['d'] = calendar_win['d'].apply(lambda x: x.split('_',2)[1]).astype(np.int16) \n","      calendar_win['event_name_1_win'] = calendar_win['event_name_1_win'].astype('category') # 转化为category，为了lgbm处理类别特征\n","      grid_df = grid_df.merge(calendar_win, on='d', how='left') # 小维度融入大维度按共有列进行merge\n","\n","      features_columns = features_columns + ['event_name_1_win'] # 72\n","      if store_id=='CA1':\n","        lgb_params = lgb_CA1_params\n","      else:\n","        lgb_params = lgb_others_params\n","    ## CA2单独有自己的节假日效应\n","    elif store_id=='CA_2':\n","      calendar_win = pd.read_csv(fea_path + 'CA2_holidays.csv')\n","      calendar_win['d'] = calendar_win['d'].apply(lambda x: x.split('_',2)[1]).astype(np.int16)\n","      calendar_win['event_name_1_win'] = calendar_win['event_name_1_win'].astype('category')\n","      grid_df = grid_df.merge(calendar_win, on='d', how='left')\n","\n","      features_columns = features_columns+['event_name_1_win']\n","      lgb_params = lgb_others_params\n","    ## TX1的节假日效应，假设所有节假日的前一天和后一天都有一定的影响\n","    elif store_id=='TX_1':\n","      calendar_df = pd.read_csv(datasets_path + 'calendar.csv')\n","      calendar_win = calendar_df[['event_name_2','event_name_1','d']]\n","      calendar_win['d'] = calendar_win['d'].apply(lambda x: x.split('_',2)[1]).astype(np.int16)\n","\n","      # 添加特征\n","      event = calendar_df[['event_name_2','event_name_1']].fillna('') # 用''替换NAN(因为NAN为float类型)\n","      event_up_one = event.shift(-1).fillna('') # 上移之后，最后一行会出现NAN，需要再fillna('')一次\n","      event_before = pd.DataFrame()\n","      event_before['event_name_2'] = event_up_one['event_name_2'].apply(lambda x: x + '_before' if x != '' else x)\n","      event_before['event_name_1'] = event_up_one['event_name_1'].apply(lambda x: x + '_before' if x != '' else x)\n","      # 总结：对Series应用匿名函数时候，这一列必须是相同的数据类型，我要实现把节假日(字符串)改名字，这一列都必须要是str，所以把 NAN 填充为空字符串！\n","      # 这样，我就可以用匿名函数，返回的x是相同的数据类型。以后对含有空值的列处理其中的字符串元素的话，就要这么做，先填充，再apply匿名函数\n","      \n","      event_down_one = event.shift(1).fillna('') # 下移之后，第一行会出现NAN，需要再fillna('')一次\n","      event_after = pd.DataFrame()\n","      event_after['event_name_2'] = event_down_one['event_name_2'].apply(lambda x: x + '_after' if x != '' else x)\n","      event_after['event_name_1'] = event_down_one['event_name_1'].apply(lambda x: x + '_after' if x != '' else x)\n","\n","      even_win = event_before + event_after # 注意只有都是字符串列也可以对dataframe相加。另外，'' + '' = ''\n","\n","      even_win['event_name_2'] = even_win['event_name_2'].apply(lambda x: np.nan if x=='' else x)\n","      even_win['event_name_1'] = even_win['event_name_1'].apply(lambda x: np.nan if x=='' else x)\n","\n","      # 去掉原来的节假日列，增加新的两列\n","      calendar_win.drop(columns=['event_name_1','event_name_2'],inplace=True) \n","      calendar_win['event_name_1_win'] = even_win['event_name_1'].astype('category') # 转化为category，lgbm方便处理\n","      calendar_win['event_name_2_win'] = even_win['event_name_2'].astype('category')\n","      calendar_win['d'] = calendar_win['d'].astype(np.int16) \n","\n","      # 融入grid_df中，74 + 2 = 76\n","      grid_df = grid_df.merge(calendar_win, on='d', how='left') # calendar_win：'d'、'event_name_1_win'、'event_name_2_win'这三列\n","      features_columns = features_columns + ['event_name_1_win','event_name_2_win'] # 71 + 2 = 73\n","\n","      # 补充的节假日效应列\n","      calendar_win_1 = pd.read_csv(fea_path + 'CA1_TX2_TX3_holidays.csv')\n","      calendar_win_1.rename(columns = {'event_name_1_win':'event_name_1_win_1'},inplace=True)\n","      calendar_win_1['d'] = calendar_win_1['d'].apply(lambda x: x.split('_',2)[1]).astype(np.int16)\n","      calendar_win_1['event_name_1_win_1'] = calendar_win_1['event_name_1_win_1'].astype('category')\n","      grid_df = grid_df.merge(calendar_win_1, on='d', how='left')\n","      features_columns = features_columns + ['event_name_1_win_1']\n","\n","      # 设置模型参数\n","      lgb_params = lgb_others_params\n","\n","    ## WI1的节假日效应，假设所有节假日的前两天和后一天都有一定的影响\n","    elif store_id=='WI_1':\n","      calendar_df = pd.read_csv(datasets_path + 'calendar.csv')\n","      calendar_win = calendar_df[['event_name_2','event_name_1','d']]\n","      calendar_win['d'] = calendar_win['d'].apply(lambda x: x.split('_',2)[1]).astype(np.int16)\n","\n","      # 添加特征\n","      event = calendar_df[['event_name_2','event_name_1']].fillna('') \n","      # 上移\n","      event_before1 = pd.DataFrame()\n","      event_up_one = event.shift(-1).fillna('') \n","      event_before1['event_name_2'] = event_up_one['event_name_2'].apply(lambda x: x + '_before1' if x != '' else x)\n","      event_before1['event_name_1'] = event_up_one['event_name_1'].apply(lambda x: x + '_before1' if x != '' else x)\n","      event_before2 = pd.DataFrame()\n","      event_up_two = event.shift(-2).fillna('')   \n","      event_before2['event_name_2'] = event_up_two['event_name_2'].apply(lambda x: x + '_before2' if x != '' else x)\n","      event_before2['event_name_1'] = event_up_two['event_name_1'].apply(lambda x: x + '_before2' if x != '' else x)\n","      event_before = event_before1 + event_before2\n","      # 下移\n","      event_after = pd.DataFrame()\n","      event_down_one = event.shift(1).fillna('') \n","      event_after['event_name_2'] = event_down_one['event_name_2'].apply(lambda x: x + '_after' if x != '' else x)\n","      event_after['event_name_1'] = event_down_one['event_name_1'].apply(lambda x: x + '_after' if x != '' else x)\n","\n","      even_win = event_before + event_after \n","\n","      # 还原NAN\n","      even_win['event_name_2'] = even_win['event_name_2'].apply(lambda x: np.nan if x=='' else x)\n","      even_win['event_name_1'] = even_win['event_name_1'].apply(lambda x: np.nan if x=='' else x)\n","      # 去掉原来的节假日列，增加新的两列\n","      calendar_win.drop(columns=['event_name_1','event_name_2'],inplace=True) \n","      calendar_win['event_name_1_win'] = even_win['event_name_1'].astype('category') # 转化为category，lgbm方便处理\n","      calendar_win['event_name_2_win'] = even_win['event_name_2'].astype('category')\n","      calendar_win['d'] = calendar_win['d'].astype(np.int16) \n","\n","      # 融入grid_df中，74 + 2 = 76\n","      grid_df = grid_df.merge(calendar_win, on='d', how='left') # calendar_win：'d'、'event_name_1_win'、'event_name_2_win'这三列\n","      features_columns = features_columns + ['event_name_1_win','event_name_2_win'] # 71 + 2 = 73\n","\n","      lgb_params = lgb_others_params\n","\n","    ## 独自的节假日效应\n","    elif store_id=='WI_3':\n","      calendar_win = pd.read_csv(fea_path + 'WI_3_holidays.csv')\n","      calendar_win['d'] = calendar_win['d'].apply(lambda x: x.split('_',2)[1]).astype(np.int16)\n","      calendar_win['event_name_1_win'] = calendar_win['event_name_1_win'].astype('category')\n","      grid_df = grid_df.merge(calendar_win, on='d', how='left')\n","      features_columns = features_columns + ['event_name_1_win']\n","      lgb_params = lgb_others_params\n","    # WI2、CA3、CA4，这三个商店只考虑所有节假日的前两天的效应\n","    else:\n","      calendar_df = pd.read_csv(datasets_path + 'calendar.csv')\n","      calendar_win = calendar_df[['event_name_2','event_name_1','d']]\n","      # 添加特征\n","      event = calendar_df[['event_name_2','event_name_1']].fillna('') \n","      # 上移\n","      event_before1 = pd.DataFrame()\n","      event_up_one = event.shift(-1).fillna('') \n","      event_before1['event_name_2'] = event_up_one['event_name_2'].apply(lambda x: x + '_before1' if x != '' else x)\n","      event_before1['event_name_1'] = event_up_one['event_name_1'].apply(lambda x: x + '_before1' if x != '' else x)\n","      event_before2 = pd.DataFrame()\n","      event_up_two = event.shift(-2).fillna('')   \n","      event_before2['event_name_2'] = event_up_two['event_name_2'].apply(lambda x: x + '_before2' if x != '' else x)\n","      event_before2['event_name_1'] = event_up_two['event_name_1'].apply(lambda x: x + '_before2' if x != '' else x)\n","      event_win = event_before1 + event_before2\n","\n","      calendar_win.drop(columns=['event_name_1','event_name_2'],inplace=True)\n","      # 转换数据类型\n","      calendar_win['event_name_1_win'] = event_win['event_name_1'].astype('category')\n","      calendar_win['event_name_2_win'] = event_win['event_name_2'].astype('category')\n","      calendar_win['d'] = calendar_win['d'].apply(lambda x: x.split('_',2)[1]).astype(np.int16)\n","\n","      grid_df = grid_df.merge(calendar_win, on='d', how='left')\n","      features_columns = features_columns + ['event_name_1_win','event_name_2_win']\n","\n","      lgb_params = lgb_others_params\n","\n","    # 加入节假日特征之后的features_columns和grid_df\n","    print(features_columns) # CA1：72\n","    print(grid_df.shape) # CA1：(4873639, 75)\n","\n","    train_mask = grid_df['d'] <= END_TRAIN - P_HORIZON # 1<= train <=1941-28 \n","    valid_mask = (grid_df['d'] > (END_TRAIN - P_HORIZON)) & (grid_df['d'] <= END_TRAIN) # 1941-28< valid <=1941\n","    preds_mask = grid_df['d'] > (END_TRAIN - 100) \n","    # 1941-100 < pred <= 1969 这128天的数据用于循环迭代预测，因为特征需要用到前面一些天销量求mean、std\n","\n","    ## 由于每个商店的节假日效应加的特征不一样，需要分开保存下来\n","    features_columns = pd.Series(features_columns) # 不转化为Series，会出现BUG：'list' object has no attribute 'to_pickle'\n","    features_columns.to_pickle(model_pkl_path +'{}_train_Fea_A.pkl'.format(store_id)) # 存储训练模型时用的特征\n","    train_data = lgb.Dataset(grid_df[train_mask][features_columns], label = grid_df[train_mask][TARGET]) # grid_df[train_mask] 返回满足train_mask的df，再取df中的features_columns特征\n","    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], label = grid_df[valid_mask][TARGET]) # 1914-1941这28天作为验证集\n","    \n","    # 构造一个后面预测用的数据集 \n","    grid_df = grid_df[preds_mask].reset_index(drop=True)\n","    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col] # '_tmp_' 滞后 + rolling 比如：rolling_mean_tmp_14_60 去掉了12列\n","    grid_df = grid_df[keep_cols] # CA1：75 - 12 = 63 (390272, 63)  [1842, 1969] 128天\n","    grid_df.to_pickle(model_pkl_path + 'test_{}_A.pkl'.format(store_id))\n","    \n","    seed_everything(SEED)\n","    # 训练模型\n","    estimator = lgb.train(lgb_params, train_data, valid_sets = [valid_data], verbose_eval = 100)\n","    # 存储模型为二进制文件，下次读取的时候，速度更快\n","    model_name = model_pkl_path + 'lgb_model_{}_v{}_A.bin'.format(store_id, str(VER))\n","    pickle.dump(estimator, open(model_name, 'wb'))\n","\n","    del grid_df, train_data, valid_data, estimator\n","    gc.collect()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Train TX_2\n","['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win']\n","(4893253, 75)\n","[100]\tvalid_0's rmse: 1.72986\n","[200]\tvalid_0's rmse: 1.71031\n","[300]\tvalid_0's rmse: 1.69986\n","[400]\tvalid_0's rmse: 1.6966\n","[500]\tvalid_0's rmse: 1.69604\n","[600]\tvalid_0's rmse: 1.6954\n","[700]\tvalid_0's rmse: 1.69554\n","[800]\tvalid_0's rmse: 1.6958\n","[900]\tvalid_0's rmse: 1.69514\n","[1000]\tvalid_0's rmse: 1.69494\n","[1100]\tvalid_0's rmse: 1.69603\n","[1200]\tvalid_0's rmse: 1.69664\n","[1300]\tvalid_0's rmse: 1.69693\n","[1400]\tvalid_0's rmse: 1.69711\n","Train TX_3\n","['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win']\n","(4822539, 75)\n","[100]\tvalid_0's rmse: 1.69849\n","[200]\tvalid_0's rmse: 1.69032\n","[300]\tvalid_0's rmse: 1.68805\n","[400]\tvalid_0's rmse: 1.68633\n","[500]\tvalid_0's rmse: 1.68626\n","[600]\tvalid_0's rmse: 1.68736\n","[700]\tvalid_0's rmse: 1.68765\n","[800]\tvalid_0's rmse: 1.6878\n","[900]\tvalid_0's rmse: 1.68841\n","[1000]\tvalid_0's rmse: 1.68866\n","[1100]\tvalid_0's rmse: 1.68933\n","[1200]\tvalid_0's rmse: 1.69015\n","[1300]\tvalid_0's rmse: 1.69014\n","[1400]\tvalid_0's rmse: 1.69057\n","Train WI_1\n","['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win', 'event_name_2_win']\n","(4646139, 76)\n","[100]\tvalid_0's rmse: 1.6036\n","[200]\tvalid_0's rmse: 1.59035\n","[300]\tvalid_0's rmse: 1.5884\n","[400]\tvalid_0's rmse: 1.58769\n","[500]\tvalid_0's rmse: 1.58725\n","[600]\tvalid_0's rmse: 1.5867\n","[700]\tvalid_0's rmse: 1.5863\n","[800]\tvalid_0's rmse: 1.58631\n","[900]\tvalid_0's rmse: 1.58624\n","[1000]\tvalid_0's rmse: 1.586\n","[1100]\tvalid_0's rmse: 1.58618\n","[1200]\tvalid_0's rmse: 1.58607\n","[1300]\tvalid_0's rmse: 1.5862\n","[1400]\tvalid_0's rmse: 1.58616\n","Train WI_2\n","['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win', 'event_name_2_win']\n","(4731952, 76)\n","[100]\tvalid_0's rmse: 2.7072\n","[200]\tvalid_0's rmse: 2.62201\n","[300]\tvalid_0's rmse: 2.60616\n","[400]\tvalid_0's rmse: 2.59623\n","[500]\tvalid_0's rmse: 2.58989\n","[600]\tvalid_0's rmse: 2.58206\n","[700]\tvalid_0's rmse: 2.57806\n","[800]\tvalid_0's rmse: 2.57566\n","[900]\tvalid_0's rmse: 2.57626\n","[1000]\tvalid_0's rmse: 2.57508\n","[1100]\tvalid_0's rmse: 2.57467\n","[1200]\tvalid_0's rmse: 2.5727\n","[1300]\tvalid_0's rmse: 2.57401\n","[1400]\tvalid_0's rmse: 2.57424\n","Train WI_3\n","['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win']\n","(4857413, 75)\n","[100]\tvalid_0's rmse: 1.93776\n","[200]\tvalid_0's rmse: 1.86808\n","[300]\tvalid_0's rmse: 1.85431\n","[400]\tvalid_0's rmse: 1.85064\n","[500]\tvalid_0's rmse: 1.85015\n","[600]\tvalid_0's rmse: 1.84978\n","[700]\tvalid_0's rmse: 1.8488\n","[800]\tvalid_0's rmse: 1.84846\n","[900]\tvalid_0's rmse: 1.84813\n","[1000]\tvalid_0's rmse: 1.8478\n","[1100]\tvalid_0's rmse: 1.84831\n","[1200]\tvalid_0's rmse: 1.8501\n","[1300]\tvalid_0's rmse: 1.851\n","[1400]\tvalid_0's rmse: 1.85101\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K-6G3-elfFma","colab_type":"text"},"source":["## 预测"]},{"cell_type":"code","metadata":{"id":"Ua8fJmu1c-qi","colab_type":"code","colab":{}},"source":["## 如果前面训练断开了，方便下面预测直接使用\n","CA1 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win']\n","CA1 = pd.Series(CA1) ## 转化为Series，才能用to_pickle()\n","CA1.to_pickle(model_pkl_path +'CA_1_train_Fea.pkl')\n","\n","CA2 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win']\n","CA2 = pd.Series(CA2)\n","CA2.to_pickle(model_pkl_path +'CA_2_train_Fea.pkl')\n","\n","CA3 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win', 'event_name_2_win']\n","CA3 = pd.Series(CA3)\n","CA3.to_pickle(model_pkl_path +'CA_3_train_Fea.pkl')\n","\n","CA4 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win', 'event_name_2_win']\n","CA4 = pd.Series(CA4)\n","CA4.to_pickle(model_pkl_path +'CA_4_train_Fea.pkl')\n","\n","TX1 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win', 'event_name_2_win', 'event_name_1_win_1']\n","TX1 = pd.Series(TX1)\n","TX1.to_pickle(model_pkl_path +'TX_1_train_Fea.pkl')\n","\n","TX2 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win']\n","TX2 = pd.Series(TX2)\n","TX2.to_pickle(model_pkl_path +'TX_2_train_Fea.pkl')\n","\n","TX3 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win']\n","TX3 = pd.Series(TX3)\n","TX3.to_pickle(model_pkl_path +'TX_3_train_Fea.pkl')\n","\n","WI1 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win', 'event_name_2_win']\n","WI1 = pd.Series(WI1)\n","WI1.to_pickle(model_pkl_path +'WI_1_train_Fea.pkl')\n","\n","WI2 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win', 'event_name_2_win']\n","WI2 = pd.Series(WI2)\n","WI2.to_pickle(model_pkl_path +'WI_2_train_Fea.pkl')\n","\n","WI3 = ['item_id', 'dept_id', 'cat_id', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'event_name_1_win']\n","WI3 = pd.Series(WI3)\n","WI3.to_pickle(model_pkl_path +'WI_3_train_Fea.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bV36FmMfPuOR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594305096616,"user_tz":-480,"elapsed":1211148,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"6d8eb208-0dd2-4692-b556-8c0c1a4856c7"},"source":["USE_AUX = True\n","END_TRAIN = 1913\n","all_preds = pd.DataFrame()\n","STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']\n","base_test = get_base_test() \n","# 3902720 rows × 66 columns 62 + 4（event_name_1_win、store_id、event_name_2_win、event_name_1_win_1）\n","# 记住：后面这个特征是新加的，原来只有62 + 12(lag+rolling) = 72\n","main_time = time.time()\n","\n","for PREDICT_DAY in range(1,29):    \n","  print('Predict | Day:', PREDICT_DAY)\n","  start_time = time.time() \n","  grid_df = base_test.copy() \n","  grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1) \n","  # 3902720(30490×128) × 78(66 + 12(lag+rolling)) \n","  # rolling_mean_tmp_14_60 每组128个数据，相当于每个id的128天这个时序，lag = 14，到1942到1942+14天有数据\n","  # 具体看一下 datasets 文件夹里面的 HOUSEHOLD_2_490_WI_3.csv文件 \n","  for store_id in STORES_IDS:\n","    model_bin = 'lgb_model_{}_v{}_A.bin'.format(store_id, str(VER))\n","    if USE_AUX:\n","      model_path = model_pkl_path + model_bin\n","    estimator = pickle.load(open(model_path, 'rb')) # 读取训练好的模型\n","    MODEL_FEATURES = pd.read_pickle(model_pkl_path +'{}_train_Fea_A.pkl'.format(store_id)).tolist() # read_pickle得到的Series，需要转化为列表\n","    ## 某一天\n","    day_mask = base_test['d'] == (END_TRAIN + PREDICT_DAY) # 1942、1943、...、1969\n","    ## 某个店\n","    store_mask = base_test['store_id'] == store_id\n","    ## 某一天某个店\n","    mask = (day_mask) & (store_mask)\n","    ## 预测某商店在 END_TRAIN + PREDICT_DAY 这一天商品的销量\n","    base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n","    ## 这里当时出现了一个BUG，说训练集和验证集的类别变量跟测试集不一致，然后我查看一下grid_df.info()发现\n","    ## 里面的节假日效应在构造测试集的时候没有转化为\"category\"，然后我就在get_base_test()里面把它们转化为类别变量，这样就可以预测了\n","\n","  temp_df = base_test[day_mask][['id',TARGET]]\n","  temp_df.columns = ['id', 'F' + str(PREDICT_DAY)]\n","  if 'id' in list(all_preds):\n","    all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n","  else:\n","    all_preds = temp_df.copy()\n","      \n","  print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n","          ' %0.2f min total |' % ((time.time() - main_time) / 60),\n","          ' %0.2f day sales |' % (temp_df['F' + str(PREDICT_DAY)].sum()))\n","  del temp_df\n","    \n","all_preds = all_preds.reset_index(drop=True)\n","all_preds.to_csv(pred_path + 'per_store_pred(lgb)_A.csv',index=False)\n","\n","# 用于生成提交预测的文件\n","submission = pd.read_csv(datasets_path + 'sample_submission.csv')[['id']] # [[]] 返回DataFrame [] 返回Series\n","submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n","submission.to_csv(sub_path + 'submission_by_stores(lgb)_A.csv', index=False) "],"execution_count":7,"outputs":[{"output_type":"stream","text":["Predict | Day: 1\n","##########  0.88 min round |  0.88 min total |  37013.82 day sales |\n","Predict | Day: 2\n","##########  0.76 min round |  1.65 min total |  34894.94 day sales |\n","Predict | Day: 3\n","##########  0.76 min round |  2.41 min total |  34696.06 day sales |\n","Predict | Day: 4\n","##########  0.72 min round |  3.13 min total |  34444.63 day sales |\n","Predict | Day: 5\n","##########  0.68 min round |  3.81 min total |  41365.51 day sales |\n","Predict | Day: 6\n","##########  0.67 min round |  4.49 min total |  50528.87 day sales |\n","Predict | Day: 7\n","##########  0.68 min round |  5.16 min total |  53052.99 day sales |\n","Predict | Day: 8\n","##########  0.67 min round |  5.83 min total |  43980.02 day sales |\n","Predict | Day: 9\n","##########  0.67 min round |  6.50 min total |  44187.72 day sales |\n","Predict | Day: 10\n","##########  0.67 min round |  7.17 min total |  40143.46 day sales |\n","Predict | Day: 11\n","##########  0.68 min round |  7.85 min total |  40617.35 day sales |\n","Predict | Day: 12\n","##########  0.68 min round |  8.53 min total |  44348.55 day sales |\n","Predict | Day: 13\n","##########  0.67 min round |  9.20 min total |  53119.12 day sales |\n","Predict | Day: 14\n","##########  0.67 min round |  9.88 min total |  45939.49 day sales |\n","Predict | Day: 15\n","##########  0.68 min round |  10.55 min total |  45078.27 day sales |\n","Predict | Day: 16\n","##########  0.68 min round |  11.23 min total |  39317.44 day sales |\n","Predict | Day: 17\n","##########  0.74 min round |  11.97 min total |  40464.17 day sales |\n","Predict | Day: 18\n","##########  0.78 min round |  12.75 min total |  41152.18 day sales |\n","Predict | Day: 19\n","##########  0.79 min round |  13.54 min total |  43937.06 day sales |\n","Predict | Day: 20\n","##########  0.75 min round |  14.29 min total |  53742.08 day sales |\n","Predict | Day: 21\n","##########  0.69 min round |  14.98 min total |  56391.64 day sales |\n","Predict | Day: 22\n","##########  0.68 min round |  15.66 min total |  42174.04 day sales |\n","Predict | Day: 23\n","##########  0.69 min round |  16.35 min total |  38128.63 day sales |\n","Predict | Day: 24\n","##########  0.68 min round |  17.03 min total |  37287.29 day sales |\n","Predict | Day: 25\n","##########  0.68 min round |  17.71 min total |  36997.72 day sales |\n","Predict | Day: 26\n","##########  0.67 min round |  18.39 min total |  41654.00 day sales |\n","Predict | Day: 27\n","##########  0.68 min round |  19.06 min total |  50776.60 day sales |\n","Predict | Day: 28\n","##########  0.68 min round |  19.74 min total |  51946.71 day sales |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tc4W3-XD6sNM","colab_type":"text"},"source":["## 某个商店进行贝叶斯调参"]},{"cell_type":"code","metadata":{"id":"VXHlTryV7POW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":199},"outputId":"ab918139-c516-42c4-b66b-a6f1951a6dae"},"source":["STORES_IDS = ['CA_1']\n","grid_df, features_columns = get_data_by_store('CA_1')\n","day = 1941\n","\n","grid_df1 = grid_df[grid_df['d'] <= day]\n","del grid_df\n","!pip install bayesian-optimization\n","\n","from bayes_opt import BayesianOptimization\n","\n","def rmse(y, y_pred):\n","  return np.sqrt(np.mean(np.square(y - y_pred)))\n","df = grid_df1\n","fe = features_columns\n","# 1000到1941-28-28 作为训练集\n","tr_x, tr_y = df[(df['d'] >= 1000) & (df['d'] <= (day-28-28))][fe], df[(df['d'] >= 1000) & (df['d'] <= (day-28-28))]['sales'] \n","# 1941-28-28到1941-28 作为测试集\n","vl_x, vl_y = df[(df['d'] > (day-28-28)) & (df['d'] <= (day-28))][fe], df[(df['d'] > (day-28-28)) & (df['d'] <= (day-28))]['sales']\n","\n","train_data = lgb.Dataset(tr_x, label=tr_y)\n","valid_data = lgb.Dataset(vl_x, label=vl_y)\n","\n","# 1914到1941 作为测试集，用训练的模型预测它的\"sales\"，再与真实的\"sales\"计算RMSE\n","test_df = df[df['d']>(day-28)].reset_index(drop=True)\n","\n","# 定义黑盒函数\n","def lgb_cv(tweedie_variance_power, subsample, subsample_freq, learning_rate, num_leaves, min_data_in_leaf,\n","      feature_fraction, max_bin, n_estimators, lambda_l2, sub_row, sub_feature, bagging_freq):\n","\n","\n","    lgb_params = {\n","                    'boosting_type': 'gbdt',\n","                    'objective': 'tweedie',\n","                    'tweedie_variance_power': tweedie_variance_power,\n","                    'metric': 'rmse',\n","                    'subsample': subsample,\n","                    'subsample_freq': int(subsample_freq),\n","                    'learning_rate': learning_rate,\n","                    'num_leaves': int(num_leaves),\n","                    'min_data_in_leaf': int(min_data_in_leaf),\n","                    'feature_fraction': feature_fraction,\n","                    'max_bin': int(max_bin),\n","                    'n_estimators': int(n_estimators),\n","                    'boost_from_average': False,\n","                    'seed': 42,  \n","                    'lambda_l2':lambda_l2,\n","                    'sub_row':sub_row,  \n","                    'sub_feature':sub_feature,\n","                    'bagging_freq':int(bagging_freq),               \n","                }\n","\n","    stimator = lgb.train(lgb_params, train_data) \n","    # valid_sets = [train_data, valid_data] 这个是用来显示training's rmse和valid_1's rmse，而 verbose_eval : 迭代多少次打印\n","\n","    test_df['preds'] = stimator.predict(test_df[fe])\n","    base_score = rmse(test_df['sales'], test_df['preds'])\n","\n","    return -base_score\n","\n","# 给定超参数搜索空间\n","opt = BayesianOptimization(\n","                lgb_cv,\n","                {\n","                    'tweedie_variance_power': (1, 1.5),\n","                    'subsample': (0.5, 1.0),\n","                    'subsample_freq': (0, 5),\n","                    'learning_rate': (0, 1),\n","                    'num_leaves':(2**10-1,2**15-1),\n","                    'min_data_in_leaf':(2**10-1,2**15-1),\n","                    'feature_fraction':(0.4,1),\n","                    'max_bin':(80,150),\n","                    'n_estimators':(1000,1700),\n","                    'lambda_l2':(0,0.2),\n","                    'sub_row':(0.6,1),\n","                    'sub_feature':(0.6,1.0),\n","                    'bagging_freq':(0,5),\n","                }\n","              )\n","opt.maximize(n_iter = 20) # 最大化黑盒函数，迭代100次\n","\n","rf_bo.max # 返回黑盒函数值最大的超参数"],"execution_count":null,"outputs":[{"output_type":"stream","text":["|   iter    |  target   | baggin... | featur... | lambda_l2 | learni... |  max_bin  | min_da... | n_esti... | num_le... | sub_fe... |  sub_row  | subsample | subsam... | tweedi... |\n","-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","| \u001b[0m 1       \u001b[0m | \u001b[0m-2.005   \u001b[0m | \u001b[0m 0.4722  \u001b[0m | \u001b[0m 0.4987  \u001b[0m | \u001b[0m 0.0196  \u001b[0m | \u001b[0m 0.03115 \u001b[0m | \u001b[0m 87.66   \u001b[0m | \u001b[0m 6.255e+0\u001b[0m | \u001b[0m 1.438e+0\u001b[0m | \u001b[0m 7.805e+0\u001b[0m | \u001b[0m 0.7139  \u001b[0m | \u001b[0m 0.7916  \u001b[0m | \u001b[0m 0.5068  \u001b[0m | \u001b[0m 0.9146  \u001b[0m | \u001b[0m 1.496   \u001b[0m |\n","| \u001b[0m 2       \u001b[0m | \u001b[0m-2.31    \u001b[0m | \u001b[0m 4.875   \u001b[0m | \u001b[0m 0.4995  \u001b[0m | \u001b[0m 0.138   \u001b[0m | \u001b[0m 0.4799  \u001b[0m | \u001b[0m 145.5   \u001b[0m | \u001b[0m 4.86e+03\u001b[0m | \u001b[0m 1.511e+0\u001b[0m | \u001b[0m 1.923e+0\u001b[0m | \u001b[0m 0.7036  \u001b[0m | \u001b[0m 0.8444  \u001b[0m | \u001b[0m 0.776   \u001b[0m | \u001b[0m 1.481   \u001b[0m | \u001b[0m 1.052   \u001b[0m |\n","| \u001b[0m 3       \u001b[0m | \u001b[0m-2.142   \u001b[0m | \u001b[0m 0.554   \u001b[0m | \u001b[0m 0.4238  \u001b[0m | \u001b[0m 0.09402 \u001b[0m | \u001b[0m 0.8591  \u001b[0m | \u001b[0m 118.5   \u001b[0m | \u001b[0m 2.158e+0\u001b[0m | \u001b[0m 1.074e+0\u001b[0m | \u001b[0m 2.461e+0\u001b[0m | \u001b[0m 0.6854  \u001b[0m | \u001b[0m 0.68    \u001b[0m | \u001b[0m 0.5021  \u001b[0m | \u001b[0m 4.48    \u001b[0m | \u001b[0m 1.46    \u001b[0m |\n","| \u001b[0m 4       \u001b[0m | \u001b[0m-2.125   \u001b[0m | \u001b[0m 2.975   \u001b[0m | \u001b[0m 0.5173  \u001b[0m | \u001b[0m 0.1388  \u001b[0m | \u001b[0m 0.1773  \u001b[0m | \u001b[0m 95.7    \u001b[0m | \u001b[0m 3.527e+0\u001b[0m | \u001b[0m 1.179e+0\u001b[0m | \u001b[0m 1.72e+04\u001b[0m | \u001b[0m 0.695   \u001b[0m | \u001b[0m 0.8235  \u001b[0m | \u001b[0m 0.7203  \u001b[0m | \u001b[0m 4.711   \u001b[0m | \u001b[0m 1.236   \u001b[0m |\n","| \u001b[0m 5       \u001b[0m | \u001b[0m-10.6    \u001b[0m | \u001b[0m 4.862   \u001b[0m | \u001b[0m 0.8312  \u001b[0m | \u001b[0m 0.03296 \u001b[0m | \u001b[0m 0.3549  \u001b[0m | \u001b[0m 147.1   \u001b[0m | \u001b[0m 2.911e+0\u001b[0m | \u001b[0m 1.499e+0\u001b[0m | \u001b[0m 3.444e+0\u001b[0m | \u001b[0m 0.9874  \u001b[0m | \u001b[0m 0.7305  \u001b[0m | \u001b[0m 0.6971  \u001b[0m | \u001b[0m 4.234   \u001b[0m | \u001b[0m 1.325   \u001b[0m |\n","| \u001b[0m 6       \u001b[0m | \u001b[0m-2.006   \u001b[0m | \u001b[0m 3.012   \u001b[0m | \u001b[0m 0.6362  \u001b[0m | \u001b[0m 0.09189 \u001b[0m | \u001b[0m 0.01151 \u001b[0m | \u001b[0m 112.2   \u001b[0m | \u001b[0m 3.139e+0\u001b[0m | \u001b[0m 1.102e+0\u001b[0m | \u001b[0m 1.871e+0\u001b[0m | \u001b[0m 0.7395  \u001b[0m | \u001b[0m 0.9646  \u001b[0m | \u001b[0m 0.9989  \u001b[0m | \u001b[0m 1.69    \u001b[0m | \u001b[0m 1.39    \u001b[0m |\n","| \u001b[0m 7       \u001b[0m | \u001b[0m-2.131   \u001b[0m | \u001b[0m 2.602   \u001b[0m | \u001b[0m 0.6728  \u001b[0m | \u001b[0m 0.05175 \u001b[0m | \u001b[0m 0.8602  \u001b[0m | \u001b[0m 100.1   \u001b[0m | \u001b[0m 2.306e+0\u001b[0m | \u001b[0m 1.195e+0\u001b[0m | \u001b[0m 1.877e+0\u001b[0m | \u001b[0m 0.7199  \u001b[0m | \u001b[0m 0.9594  \u001b[0m | \u001b[0m 0.5521  \u001b[0m | \u001b[0m 0.3486  \u001b[0m | \u001b[0m 1.116   \u001b[0m |\n"],"name":"stdout"}]}]}