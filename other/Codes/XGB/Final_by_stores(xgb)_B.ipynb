{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_by_stores(xgb)_B.ipynb","provenance":[],"collapsed_sections":["tc4W3-XD6sNM"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"z_5SVT6RvYYq","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1594725424765,"user_tz":-480,"elapsed":15626,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"e0e7ef3d-1644-42d3-d62f-dd262603ae47"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jiFkJMb3Ey0F"},"source":["import numpy as np\n","import pandas as pd\n","import os, sys, gc, time, warnings, pickle, psutil, random\n","import lightgbm as lgb\n","from multiprocessing import Pool        \n","warnings.filterwarnings('ignore')\n","\n","fea_pkl_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/pkl/fea_pkl/'\n","model_pkl_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/pkl/model_pkl/'\n","Metric_pkl_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/pkl/Metric_pkl/'\n","datasets_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/datasets/'\n","fea_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/Features/'\n","pred_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/Predict/'\n","sub_path = '/content/drive/My Drive/m5-forecasting-accuracy/SilverCode(final)/Predict/sub/'\n","\n","\n","BASE = fea_pkl_path + 'grid_part_1.pkl'\n","PRICE = fea_pkl_path + 'grid_part_2.pkl'\n","CALENDAR = fea_pkl_path + 'grid_part_3.pkl'\n","LAGS = fea_pkl_path + 'grid_part_4.pkl'\n","MEAN_ENC = fea_pkl_path + 'grid_part_5.pkl'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhPOT38tOl_q"},"source":["## 设定随机种子\n","def seed_everything(seed=0):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","\n","## 按照商店读取数据\n","def get_data_by_store(store):\n","    # 读取和连接基本特征\n","    df = pd.concat([pd.read_pickle(BASE), # 9\n","             pd.read_pickle(PRICE).iloc[:,2:], # 11\n","             pd.read_pickle(CALENDAR).iloc[:,2:], # 15 \n","             pd.read_pickle(LAGS).iloc[:,3:], # 37\n","             pd.read_pickle(MEAN_ENC)[mean_features]], # 6  \n","             axis=1) \n","    # df：(47735397, 78)由于行数相同，可以横向拼接\n","    df = df[df['store_id'] == store] # df['store_id'] == store 返回布尔Series，df[df['store_id'] == store] 然后取True的行构成的dataframe\n","    features = [col for col in list(df) if col not in remove_features] # 78 - 7 = 71\n","    df = df[['id', 'd', TARGET] + features] # 71 + 3 = 74 相当于去掉了 'state_id', 'store_id', 'release', 'Holiday'(在CALENDAR里面)\n","    # 选训练开始的天数\n","    df = df[df['d'] >= START_TRAIN].reset_index(drop=True) # drop = True去掉生成的原索引列\n","    return df, features\n","\n","\n","## 读取测试数据\n","def get_base_test():\n","  base_test = pd.DataFrame()\n","\n","  if USE_AUX:\n","    model_path = model_pkl_path\n","  else:\n","    model_path=''\n","\n","  for store_id in STORES_IDS: \n","    temp_df = pd.read_pickle(model_path + 'xgb_test_{}_B.pkl'.format(store_id)) # 不含lag + rolling特征\n","    temp_df['store_id'] = store_id\n","    base_test = pd.concat([base_test, temp_df]).reset_index(drop=True) # 纵向拼接\n","  \n","  return base_test\n","## 制作lag特征\n","def make_lag(LAG_DAY):\n","  lag_df = base_test[['id','d',TARGET]]\n","  col_name = 'sales_lag_' + str(LAG_DAY)\n","  lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n","  return lag_df[[col_name]]\n","## 递归特征\n","def make_lag_roll(LAG_DAY):\n","  shift_day = LAG_DAY[0]\n","  roll_wind = LAG_DAY[1]\n","  lag_df = base_test[['id','d',TARGET]] \n","  col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n","  lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean()) \n","\n","  return lag_df[[col_name]]\n","## 多线程执行，用于测试集融合\n","def df_parallelize_run(func, t_split):\n","  num_cores = np.min([N_CORES,len(t_split)])\n","  pool = Pool(num_cores)\n","  df = pd.concat(pool.map(func, t_split), axis=1)\n","  pool.close()\n","  pool.join()\n","  return df\n","\n","# lag + rolling\n","SHIFT_DAY = 28\n","N_LAGS = 15\n","LAGS_SPLIT = [col for col in range(SHIFT_DAY, SHIFT_DAY + N_LAGS)]\n","ROLS_SPLIT = []\n","for i in [1,7,14]:\n","  for j in [7,14,30,60]:\n","    ROLS_SPLIT.append([i,j])\n","\n","# grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n","\n","\n","VER = 1 # 设置模型的版本\n","SEED = 42 \n","seed_everything(SEED) # 消除随机性\n","N_CORES = psutil.cpu_count() # 可使用的CPU内核\n","\n","TARGET = 'sales' # Label\n","# START_TRAIN = 1600 # 快速训练，验证是否有BUG\n","START_TRAIN = 0 # 真正训练\n","END_TRAIN = 1941\n","P_HORIZON = 28 # 预测范围\n","USE_AUX = True # 使用预训练好的模型\n","\n","remove_features = ['id', 'state_id', 'store_id', 'release', 'Holiday', 'd', TARGET]          \n","mean_features = ['enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std'] \n","# 按商店分别训练，每个商店可以只能按类别、部门、商品的销量聚合取mean\\std，故只选这6个特征\n","STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_R3Odek2Vl1"},"source":["## 每个商店的xgb参数及类别特征处理"]},{"cell_type":"code","metadata":{"id":"TbHapyQzzrcE"},"source":["xgb_params_common = {\n","            'booster': 'gbtree',\n","            'objective': 'reg:tweedie',\n","            'eval_metric': 'rmse',\n","            'min_data_in_leaf': 11801,\n","            'lambda': 0.021982796763644744, \n","            'eta': 0.04723157294394453,\n","            'colsample_bytree': 0.9329928507911868, \n","            'subsample': 0.5951931300022044,    \n","            'max_bin':67,\n","            'num_leaves': 25631,\n","            'sub_row': 0.6184421797679618,\n","            'subsample_freq': 3,\n","            'silent':1, \n","            'seed': 42,\n","            'tree_method': 'hist',\n","            'tweedie_variance_power':1.1833186379351004,\n","            'boost_from_average': False\n","           }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RyUc8phrWmGF","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594725434381,"user_tz":-480,"elapsed":2502,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"dd57915b-e6ab-4b71-c14a-c4b950fdb00a"},"source":["# 下面处理类别特征需要用到的工具包\n","import xgboost as xgb\n","from sklearn.decomposition import PCA\n","from sklearn import preprocessing\n","from keras.layers.embeddings import Embedding\n","from keras.models import Sequential\n","import tensorflow as tf\n","import random as rn\n","import os\n","os.environ['PYTHONHASHSEED'] = '0'\n","np.random.seed(42)\n","rn.seed(12345)\n","session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"52JVXYxX2heR"},"source":["## 每个商店单独训练模型"]},{"cell_type":"code","metadata":{"id":"C84oJ59GPJ25","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594733524927,"user_tz":-480,"elapsed":8091764,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"e3be7eeb-f1a8-4b36-8fd8-2caafc0ccdf2"},"source":["############################################# Train Models ###############################################\n","for store_id in STORES_IDS: # 每个商店单独训练\n","    print('Train', store_id)\n","    grid_df, features_columns = get_data_by_store(store_id) \n","    calendar_win = pd.read_csv(fea_path + 'CA1_TX2_TX3_holidays.csv')\n","    calendar_win['d'] = calendar_win['d'].apply(lambda x: x.split('_',2)[1]).astype(np.int16) \n","    calendar_win['event_name_1_win'] = calendar_win['event_name_1_win'].astype('category') \n","    grid_df = grid_df.merge(calendar_win, on='d', how='left') \n","\n","    #################################### PCA ####################################\n","    pca_fea = ['snap_CA', 'snap_TX', 'snap_WI'] # 这三个分别onehot编码之后，再PCA，能保持100%的信息\n","    pca_df = pd.DataFrame()\n","    pca = PCA(n_components = 'mle')\n","    OneHot_snap = pd.get_dummies(grid_df[pca_fea])\n","    pca_df = pd.concat([pd.DataFrame(pca.fit_transform(OneHot_snap.iloc[:,0:2]), columns = ['snap_CA_pca']),\n","               pd.DataFrame(pca.fit_transform(OneHot_snap.iloc[:,2:4]), columns = ['snap_TX_pca']),\n","               pd.DataFrame(pca.fit_transform(OneHot_snap.iloc[:,4:6]), columns = ['snap_WI_pca'])], axis=1)\n","    #################################### One-Hot ####################################\n","    one_hot_fea = ['cat_id', 'event_type_2'] # 'cat_id'特征值个数为3，'event_type_2'：2\n","    one_hot_df = pd.get_dummies(grid_df[one_hot_fea])  \n","    ############################ LabelEncoder + Enbedding ##########################\n","    ## 先用label编码 \n","    embedding_fea = ['dept_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_name_1_win']\n","    for i in embedding_fea:\n","      if grid_df[i].dtypes=='category':\n","        le = preprocessing.LabelEncoder()\n","        grid_df[i] = le.fit_transform(list(grid_df[i].values))\n","\n","    intput_vec = [7,31,5,5,32]\n","    output_vec = [3,10,3,3,10]\n","    embedding_df = pd.DataFrame()\n","    int_j = 0\n","    out_k = 0\n","    ## 再用embedding进行特征降维\n","    for i in embedding_fea:\n","      model = Sequential()\n","      model.add(Embedding(intput_vec[int_j], output_vec[out_k], input_length = 1))\n","      model.compile(loss='mean_squared_error', optimizer='rmsprop')\n","      input_array = grid_df[[i]].values\n","      output_array = model.predict(input_array).reshape((-1, output_vec[out_k]))\n","      # intput_vec[int_j]：不同特征值的个数(词汇表大小) output_vec[out_k]：词向量的维度\n","      # 输入尺寸为 (batch_size, sequence_length) 的 2D 张量\n","      # 输出尺寸为 (batch_size, sequence_length, output_dim) 的 3D 张量，比如：'dept_id' embedding之后(4873639, 1, 3)，需要reshape一下。\n","      temp_df = pd.DataFrame(output_array)\n","      embedding_df = pd.concat([embedding_df, temp_df], axis = 1)\n","      int_j += 1\n","      out_k += 1\n","\n","    embedding_df.columns = [f'eme_{col}' for col in range(len(embedding_df.columns))]\n","\n","    category_feature = ['item_id', 'dept_id', 'cat_id', 'snap_CA', 'snap_TX', 'snap_WI', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'event_name_1_win'] \n","                # 由于item_id有3049个特征值，没办法处理，剔除掉，其他的分开处理\n","    grid_df.drop(columns = category_feature, inplace = True) ## BUG：不能剔除掉 \"id\" 否则后面没办法迭代按照 id 生成迭代预测的特征了。\n","    ## 融合到一起 \n","    xgb_df = pd.concat([embedding_df, one_hot_df, pca_df], axis=1)\n","    grid_df = pd.concat([grid_df, xgb_df], axis = 1)\n","    features_columns = list(set(features_columns).difference(set(category_feature))) + xgb_df.columns.to_list()\n","    # set(listA).difference(set(listB)) 去掉AB的交集，取A剩下的。set运算完之后，返回的还是set，需要转化为list\n","    del xgb_df, embedding_df, one_hot_df, pca_df\n","    print('finish：{}'.format(store_id))\n","\n","    xgb_params = xgb_params_common\n","\n","    train_mask = grid_df['d'] <= END_TRAIN - P_HORIZON # 1<= train <=1941-28 \n","    valid_mask = (grid_df['d'] > (END_TRAIN - P_HORIZON)) & (grid_df['d'] <= END_TRAIN) # 1941-28< valid <=1941\n","    preds_mask = grid_df['d'] > (END_TRAIN - 100) \n","\n","    train_data = xgb.DMatrix(grid_df[train_mask][features_columns], label = grid_df[train_mask][TARGET])\n","    valid_data = xgb.DMatrix(grid_df[valid_mask][features_columns], label = grid_df[valid_mask][TARGET])\n","\n","    # 构造一个后面预测用的数据集 \n","    grid_df = grid_df[preds_mask].reset_index(drop=True)\n","    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col] # '_tmp_' 滞后 + rolling 比如：rolling_mean_tmp_14_60 去掉了12列\n","    grid_df = grid_df[keep_cols] # CA1：75 - 12 = 63 (390272, 63)  [1842, 1969] 128天\n","    grid_df.to_pickle(model_pkl_path + 'xgb_test_{}_B.pkl'.format(store_id))\n","    \n","    # 训练模型\n","    seed_everything(SEED)\n","    watchlist = [(valid_data, 'eval')]\n","    estimator = xgb.train(xgb_params, train_data, evals = watchlist, num_boost_round = 20000, verbose_eval = 100, early_stopping_rounds = 100)\n","    # early_stopping_rounds 验证集的误差迭代到一定程度在100次内不能再继续降低，就停止迭代。要求evals里至少有一个元素。\n","    # 存储模型为二进制文件，下次读取的时候，速度更快\n","    model_name = model_pkl_path + 'xgb_model_{}_v{}_B.bin'.format(store_id, str(VER))\n","    pickle.dump(estimator, open(model_name, 'wb'))\n","\n","    del grid_df, train_data, valid_data, estimator\n","    gc.collect()\n","\n","## 遇到的问题：训练的时候只迭代几步就结束了\n","# Train CA_1\n","# finish：CA_1\n","# [0]\ttrain-rmse:4.58096\teval-rmse:3.72794\n","# [9]\ttrain-rmse:4.2113\teval-rmse:3.37202\n","# Train CA_2\n","## 解决办法：在train中设置：num_boost_round，因为没有设置迭代次数的话，默认只迭代10次就停止了。 \n","## Xgboost中的迭代次数 num_boost_round 和LightGBM中的迭代次数 n_estimators 是一样的"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train CA_1\n","finish：CA_1\n","[0]\teval-rmse:3.72797\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:2.02326\n","[200]\teval-rmse:2.01679\n","[300]\teval-rmse:2.01333\n","[400]\teval-rmse:2.01175\n","[500]\teval-rmse:2.00901\n","[600]\teval-rmse:2.00694\n","[700]\teval-rmse:2.00617\n","[800]\teval-rmse:2.00645\n","Stopping. Best iteration:\n","[787]\teval-rmse:2.0044\n","\n","Train CA_2\n","finish：CA_2\n","[0]\teval-rmse:3.30137\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:1.93221\n","[200]\teval-rmse:1.91596\n","[300]\teval-rmse:1.90753\n","[400]\teval-rmse:1.90361\n","[500]\teval-rmse:1.89982\n","[600]\teval-rmse:1.89868\n","[700]\teval-rmse:1.89746\n","[800]\teval-rmse:1.89663\n","[900]\teval-rmse:1.89545\n","[1000]\teval-rmse:1.89421\n","[1100]\teval-rmse:1.89342\n","Stopping. Best iteration:\n","[1094]\teval-rmse:1.89332\n","\n","Train CA_3\n","finish：CA_3\n","[0]\teval-rmse:5.02822\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:2.41105\n","Stopping. Best iteration:\n","[82]\teval-rmse:2.40769\n","\n","Train CA_4\n","finish：CA_4\n","[0]\teval-rmse:1.98322\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:1.39829\n","[200]\teval-rmse:1.39804\n","[300]\teval-rmse:1.39683\n","[400]\teval-rmse:1.39619\n","[500]\teval-rmse:1.39555\n","Stopping. Best iteration:\n","[489]\teval-rmse:1.39537\n","\n","Train TX_1\n","finish：TX_1\n","[0]\teval-rmse:3.16447\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:1.62729\n","[200]\teval-rmse:1.6286\n","[300]\teval-rmse:1.62573\n","[400]\teval-rmse:1.62147\n","[500]\teval-rmse:1.61727\n","[600]\teval-rmse:1.61671\n","Stopping. Best iteration:\n","[560]\teval-rmse:1.61575\n","\n","Train TX_2\n","finish：TX_2\n","[0]\teval-rmse:3.77322\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:1.78235\n","[200]\teval-rmse:1.78063\n","Stopping. Best iteration:\n","[133]\teval-rmse:1.7796\n","\n","Train TX_3\n","finish：TX_3\n","[0]\teval-rmse:3.6969\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:1.85276\n","[200]\teval-rmse:1.8419\n","[300]\teval-rmse:1.83998\n","[400]\teval-rmse:1.8394\n","Stopping. Best iteration:\n","[376]\teval-rmse:1.83834\n","\n","Train WI_1\n","finish：WI_1\n","[0]\teval-rmse:2.67602\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:1.60656\n","[200]\teval-rmse:1.60101\n","[300]\teval-rmse:1.5983\n","[400]\teval-rmse:1.59811\n","[500]\teval-rmse:1.59725\n","[600]\teval-rmse:1.59694\n","[700]\teval-rmse:1.5965\n","Stopping. Best iteration:\n","[673]\teval-rmse:1.59627\n","\n","Train WI_2\n","finish：WI_2\n","[0]\teval-rmse:5.05493\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:2.71749\n","[200]\teval-rmse:2.69301\n","[300]\teval-rmse:2.69236\n","Stopping. Best iteration:\n","[229]\teval-rmse:2.6889\n","\n","Train WI_3\n","finish：WI_3\n","[0]\teval-rmse:3.87907\n","Will train until eval-rmse hasn't improved in 100 rounds.\n","[100]\teval-rmse:1.91819\n","[200]\teval-rmse:1.90546\n","[300]\teval-rmse:1.90312\n","[400]\teval-rmse:1.896\n","[500]\teval-rmse:1.89767\n","Stopping. Best iteration:\n","[422]\teval-rmse:1.8944\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J3ty0UFVCDKh"},"source":["MODEL_FEATURES = features_columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K-6G3-elfFma"},"source":["## 预测"]},{"cell_type":"code","metadata":{"id":"bV36FmMfPuOR","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594734630098,"user_tz":-480,"elapsed":9191057,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"0435bace-4cd0-4376-e645-8bb9107eabae"},"source":["USE_AUX = True\n","all_preds = pd.DataFrame()\n","base_test = get_base_test() \n","# 3902720 rows × 66 columns 62 + 4（event_name_1_win、store_id、event_name_2_win、event_name_1_win_1）\n","# 记住：后面这个特征是新加的，原来只有62 + 12(lag+rolling) = 72\n","main_time = time.time()\n","\n","for PREDICT_DAY in range(1,29):    \n","  print('Predict | Day:', PREDICT_DAY)\n","  start_time = time.time() \n","  grid_df = base_test.copy() \n","  grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1) \n","  # print(grid_df.columns)\n","  print('finish roll')\n","  # 3902720(30490×128) × 78(66 + 12(lag+rolling)) \n","  # rolling_mean_tmp_14_60 每组128个数据，相当于每个id的128天这个时序，lag = 14，到1942到1942+14天有数据\n","  # 具体看一下 datasets 文件夹里面的 HOUSEHOLD_2_490_WI_3.csv文件 \n","  for store_id in STORES_IDS:\n","    model_bin = 'xgb_model_{}_v{}_B.bin'.format(store_id, str(VER))\n","    if USE_AUX:\n","      model_path = model_pkl_path + model_bin\n","    estimator = pickle.load(open(model_path, 'rb')) # 读取训练好的模型\n","\n","\n","    day_mask = base_test['d'] == (END_TRAIN + PREDICT_DAY) # 1942、1943、...、1969\n","    store_mask = base_test['store_id'] == store_id\n","    mask = (day_mask) & (store_mask)\n","\n","    test = xgb.DMatrix(grid_df[mask][MODEL_FEATURES]) \n","    base_test[TARGET][mask] = estimator.predict(test) # 测试集也要封装成DMatrix形式\n","\n","  temp_df = base_test[day_mask][['id',TARGET]]\n","  # print(temp_df)\n","  temp_df.columns = ['id', 'F' + str(PREDICT_DAY)]\n","  if 'id' in list(all_preds):\n","    all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n","  else:\n","    all_preds = temp_df.copy()\n","      \n","  print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n","          ' %0.2f min total |' % ((time.time() - main_time) / 60),\n","          ' %0.2f day sales |' % (temp_df['F' + str(PREDICT_DAY)].sum()))\n","  del temp_df\n","    \n","all_preds = all_preds.reset_index(drop=True)\n","all_preds.to_csv(pred_path + 'per_store_pred(xgb)_B.csv',index=False)\n","\n","\n","# 用于生成提交预测的文件\n","submission = pd.read_csv(datasets_path + 'sample_submission.csv')[['id']] \n","submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n","submission.to_csv(sub_path + 'submission_by_stores(xgb)_B.csv', index=False) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Predict | Day: 1\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.67 min round |  0.67 min total |  39163.18 day sales |\n","Predict | Day: 2\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  1.30 min total |  36000.76 day sales |\n","Predict | Day: 3\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.64 min round |  1.94 min total |  35490.93 day sales |\n","Predict | Day: 4\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.65 min round |  2.59 min total |  35675.25 day sales |\n","Predict | Day: 5\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.64 min round |  3.23 min total |  40690.81 day sales |\n","Predict | Day: 6\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  3.86 min total |  49051.84 day sales |\n","Predict | Day: 7\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.64 min round |  4.50 min total |  45559.43 day sales |\n","Predict | Day: 8\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.64 min round |  5.15 min total |  42705.26 day sales |\n","Predict | Day: 9\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  5.78 min total |  37288.89 day sales |\n","Predict | Day: 10\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  6.41 min total |  42053.20 day sales |\n","Predict | Day: 11\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  7.03 min total |  43029.08 day sales |\n","Predict | Day: 12\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  7.66 min total |  49665.51 day sales |\n","Predict | Day: 13\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  8.29 min total |  54002.67 day sales |\n","Predict | Day: 14\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  8.91 min total |  56218.90 day sales |\n","Predict | Day: 15\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  9.54 min total |  45709.01 day sales |\n","Predict | Day: 16\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.62 min round |  10.16 min total |  41185.09 day sales |\n","Predict | Day: 17\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  10.79 min total |  41367.31 day sales |\n","Predict | Day: 18\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.62 min round |  11.42 min total |  42753.96 day sales |\n","Predict | Day: 19\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  12.05 min total |  45196.03 day sales |\n","Predict | Day: 20\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  12.68 min total |  56377.73 day sales |\n","Predict | Day: 21\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.64 min round |  13.32 min total |  57258.60 day sales |\n","Predict | Day: 22\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  13.95 min total |  43514.52 day sales |\n","Predict | Day: 23\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.64 min round |  14.60 min total |  41162.62 day sales |\n","Predict | Day: 24\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.62 min round |  15.22 min total |  42051.15 day sales |\n","Predict | Day: 25\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  15.85 min total |  39409.89 day sales |\n","Predict | Day: 26\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.62 min round |  16.47 min total |  44961.99 day sales |\n","Predict | Day: 27\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.62 min round |  17.10 min total |  53072.53 day sales |\n","Predict | Day: 28\n","Index(['id', 'd', 'sales', 'sell_price', 'price_max', 'price_min', 'price_std',\n","       'price_mean', 'price_norm', 'price_nunique',\n","       ...\n","       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n","       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n","       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n","       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n","       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60'],\n","      dtype='object', length=102)\n","finish roll\n","##########  0.63 min round |  17.72 min total |  49022.25 day sales |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a9StIQuqjMaT"},"source":["################################## 本地wrmse #############################################################\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error\n","from scipy.sparse import csr_matrix\n","import gc\n","\n","# 加载前面预先计算好的各个权重\n","sw_df = pd.read_pickle(Metric_pkl_path + 'sw_df.pkl')\n","S = sw_df.s.values\n","W = sw_df.w.values\n","SW = sw_df.sw.values\n","roll_mat_df = pd.read_pickle(Metric_pkl_path + 'roll_mat_df.pkl')\n","roll_index = roll_mat_df.index\n","roll_mat_csr = csr_matrix(roll_mat_df.values)\n","\n","\n","def rollup(v):\n","    return (v.T*roll_mat_csr.T).T\n","\n","# 计算 WRMSSE 评估指标\n","def wrmsse(preds, y_true, score_only=False, s = S, w = W, sw=SW):\n","    '''\n","    preds - Predictions: pd.DataFrame of size (30490 rows, N day columns)\n","    y_true - True values: pd.DataFrame of size (30490 rows, N day columns)\n","    sequence_length - np.array of size (42840,)\n","    sales_weight - sales weights based on last 28 days: np.array (42840,)\n","    '''\n","    \n","    if score_only:\n","        return np.sum(\n","                np.sqrt(\n","                    np.mean(\n","                        np.square(rollup(preds.values-y_true.values))\n","                            ,axis=1)) * sw )*(1/12)\n","    else: \n","        score_matrix = (np.square(rollup(preds.values-y_true.values)) * np.square(w)[:, None])  / s[:, None]\n","        score = np.sum(np.sqrt(np.mean(score_matrix,axis=1)))*(1/12)\n","        return score, score_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mh_LK8yskYe_"},"source":["## 预测值\n","cols = [f'F{i}' for i in range(1,29)]\n","pred = all_preds1[cols]\n","## 真值1\n","cols_1 = [f'd_{i}' for i in range(1886, 1914)]\n","true_1 = pd.read_csv(datasets_path + 'sales_train_validation.csv')[col2]\n","## 真值2\n","cols_2 = [f'd_{i}' for i in range(1914, 1942)]\n","true_2 = pd.read_csv(datasets_path + 'sales_train_evaluation.csv')[cols_2]\n","## 进行测试\n","wrmsse(true_2, pred ,score_only=True)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tc4W3-XD6sNM"},"source":["## 某个商店进行贝叶斯调参"]},{"cell_type":"code","metadata":{"id":"VXHlTryV7POW","colab":{"base_uri":"https://localhost:8080/","height":199},"outputId":"ab918139-c516-42c4-b66b-a6f1951a6dae"},"source":["STORES_IDS = ['CA_1']\n","grid_df, features_columns = get_data_by_store('CA_1')\n","day = 1941\n","\n","grid_df1 = grid_df[grid_df['d'] <= day]\n","del grid_df\n","!pip install bayesian-optimization\n","\n","from bayes_opt import BayesianOptimization\n","\n","def rmse(y, y_pred):\n","  return np.sqrt(np.mean(np.square(y - y_pred)))\n","df = grid_df1\n","fe = features_columns\n","# 1000到1941-28-28 作为训练集\n","tr_x, tr_y = df[(df['d'] >= 1000) & (df['d'] <= (day-28-28))][fe], df[(df['d'] >= 1000) & (df['d'] <= (day-28-28))]['sales'] \n","# 1941-28-28到1941-28 作为测试集\n","vl_x, vl_y = df[(df['d'] > (day-28-28)) & (df['d'] <= (day-28))][fe], df[(df['d'] > (day-28-28)) & (df['d'] <= (day-28))]['sales']\n","\n","train_data = lgb.Dataset(tr_x, label=tr_y)\n","valid_data = lgb.Dataset(vl_x, label=vl_y)\n","\n","# 1914到1941 作为测试集，用训练的模型预测它的\"sales\"，再与真实的\"sales\"计算RMSE\n","test_df = df[df['d']>(day-28)].reset_index(drop=True)\n","\n","# 定义黑盒函数\n","def lgb_cv(tweedie_variance_power, subsample, subsample_freq, learning_rate, num_leaves, min_data_in_leaf,\n","      feature_fraction, max_bin, n_estimators, lambda_l2, sub_row, sub_feature, bagging_freq):\n","\n","\n","    lgb_params = {\n","                    'boosting_type': 'gbdt',\n","                    'objective': 'tweedie',\n","                    'tweedie_variance_power': tweedie_variance_power,\n","                    'metric': 'rmse',\n","                    'subsample': subsample,\n","                    'subsample_freq': int(subsample_freq),\n","                    'learning_rate': learning_rate,\n","                    'num_leaves': int(num_leaves),\n","                    'min_data_in_leaf': int(min_data_in_leaf),\n","                    'feature_fraction': feature_fraction,\n","                    'max_bin': int(max_bin),\n","                    'n_estimators': int(n_estimators),\n","                    'boost_from_average': False,\n","                    'seed': 42,  \n","                    'lambda_l2':lambda_l2,\n","                    'sub_row':sub_row,  \n","                    'sub_feature':sub_feature,\n","                    'bagging_freq':int(bagging_freq),               \n","                }\n","\n","    stimator = lgb.train(lgb_params, train_data) \n","    # valid_sets = [train_data, valid_data] 这个是用来显示training's rmse和valid_1's rmse，而 verbose_eval : 迭代多少次打印\n","\n","    test_df['preds'] = stimator.predict(test_df[fe])\n","    base_score = rmse(test_df['sales'], test_df['preds'])\n","\n","    return -base_score\n","\n","# 给定超参数搜索空间\n","opt = BayesianOptimization(\n","                lgb_cv,\n","                {\n","                    'tweedie_variance_power': (1, 1.5),\n","                    'subsample': (0.5, 1.0),\n","                    'subsample_freq': (0, 5),\n","                    'learning_rate': (0, 1),\n","                    'num_leaves':(2**10-1,2**15-1),\n","                    'min_data_in_leaf':(2**10-1,2**15-1),\n","                    'feature_fraction':(0.4,1),\n","                    'max_bin':(80,150),\n","                    'n_estimators':(1000,1700),\n","                    'lambda_l2':(0,0.2),\n","                    'sub_row':(0.6,1),\n","                    'sub_feature':(0.6,1.0),\n","                    'bagging_freq':(0,5),\n","                }\n","              )\n","opt.maximize(n_iter = 20) # 最大化黑盒函数，迭代100次\n","\n","rf_bo.max # 返回黑盒函数值最大的超参数"],"execution_count":null,"outputs":[{"output_type":"stream","text":["|   iter    |  target   | baggin... | featur... | lambda_l2 | learni... |  max_bin  | min_da... | n_esti... | num_le... | sub_fe... |  sub_row  | subsample | subsam... | tweedi... |\n","-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","| \u001b[0m 1       \u001b[0m | \u001b[0m-2.005   \u001b[0m | \u001b[0m 0.4722  \u001b[0m | \u001b[0m 0.4987  \u001b[0m | \u001b[0m 0.0196  \u001b[0m | \u001b[0m 0.03115 \u001b[0m | \u001b[0m 87.66   \u001b[0m | \u001b[0m 6.255e+0\u001b[0m | \u001b[0m 1.438e+0\u001b[0m | \u001b[0m 7.805e+0\u001b[0m | \u001b[0m 0.7139  \u001b[0m | \u001b[0m 0.7916  \u001b[0m | \u001b[0m 0.5068  \u001b[0m | \u001b[0m 0.9146  \u001b[0m | \u001b[0m 1.496   \u001b[0m |\n","| \u001b[0m 2       \u001b[0m | \u001b[0m-2.31    \u001b[0m | \u001b[0m 4.875   \u001b[0m | \u001b[0m 0.4995  \u001b[0m | \u001b[0m 0.138   \u001b[0m | \u001b[0m 0.4799  \u001b[0m | \u001b[0m 145.5   \u001b[0m | \u001b[0m 4.86e+03\u001b[0m | \u001b[0m 1.511e+0\u001b[0m | \u001b[0m 1.923e+0\u001b[0m | \u001b[0m 0.7036  \u001b[0m | \u001b[0m 0.8444  \u001b[0m | \u001b[0m 0.776   \u001b[0m | \u001b[0m 1.481   \u001b[0m | \u001b[0m 1.052   \u001b[0m |\n","| \u001b[0m 3       \u001b[0m | \u001b[0m-2.142   \u001b[0m | \u001b[0m 0.554   \u001b[0m | \u001b[0m 0.4238  \u001b[0m | \u001b[0m 0.09402 \u001b[0m | \u001b[0m 0.8591  \u001b[0m | \u001b[0m 118.5   \u001b[0m | \u001b[0m 2.158e+0\u001b[0m | \u001b[0m 1.074e+0\u001b[0m | \u001b[0m 2.461e+0\u001b[0m | \u001b[0m 0.6854  \u001b[0m | \u001b[0m 0.68    \u001b[0m | \u001b[0m 0.5021  \u001b[0m | \u001b[0m 4.48    \u001b[0m | \u001b[0m 1.46    \u001b[0m |\n","| \u001b[0m 4       \u001b[0m | \u001b[0m-2.125   \u001b[0m | \u001b[0m 2.975   \u001b[0m | \u001b[0m 0.5173  \u001b[0m | \u001b[0m 0.1388  \u001b[0m | \u001b[0m 0.1773  \u001b[0m | \u001b[0m 95.7    \u001b[0m | \u001b[0m 3.527e+0\u001b[0m | \u001b[0m 1.179e+0\u001b[0m | \u001b[0m 1.72e+04\u001b[0m | \u001b[0m 0.695   \u001b[0m | \u001b[0m 0.8235  \u001b[0m | \u001b[0m 0.7203  \u001b[0m | \u001b[0m 4.711   \u001b[0m | \u001b[0m 1.236   \u001b[0m |\n","| \u001b[0m 5       \u001b[0m | \u001b[0m-10.6    \u001b[0m | \u001b[0m 4.862   \u001b[0m | \u001b[0m 0.8312  \u001b[0m | \u001b[0m 0.03296 \u001b[0m | \u001b[0m 0.3549  \u001b[0m | \u001b[0m 147.1   \u001b[0m | \u001b[0m 2.911e+0\u001b[0m | \u001b[0m 1.499e+0\u001b[0m | \u001b[0m 3.444e+0\u001b[0m | \u001b[0m 0.9874  \u001b[0m | \u001b[0m 0.7305  \u001b[0m | \u001b[0m 0.6971  \u001b[0m | \u001b[0m 4.234   \u001b[0m | \u001b[0m 1.325   \u001b[0m |\n","| \u001b[0m 6       \u001b[0m | \u001b[0m-2.006   \u001b[0m | \u001b[0m 3.012   \u001b[0m | \u001b[0m 0.6362  \u001b[0m | \u001b[0m 0.09189 \u001b[0m | \u001b[0m 0.01151 \u001b[0m | \u001b[0m 112.2   \u001b[0m | \u001b[0m 3.139e+0\u001b[0m | \u001b[0m 1.102e+0\u001b[0m | \u001b[0m 1.871e+0\u001b[0m | \u001b[0m 0.7395  \u001b[0m | \u001b[0m 0.9646  \u001b[0m | \u001b[0m 0.9989  \u001b[0m | \u001b[0m 1.69    \u001b[0m | \u001b[0m 1.39    \u001b[0m |\n","| \u001b[0m 7       \u001b[0m | \u001b[0m-2.131   \u001b[0m | \u001b[0m 2.602   \u001b[0m | \u001b[0m 0.6728  \u001b[0m | \u001b[0m 0.05175 \u001b[0m | \u001b[0m 0.8602  \u001b[0m | \u001b[0m 100.1   \u001b[0m | \u001b[0m 2.306e+0\u001b[0m | \u001b[0m 1.195e+0\u001b[0m | \u001b[0m 1.877e+0\u001b[0m | \u001b[0m 0.7199  \u001b[0m | \u001b[0m 0.9594  \u001b[0m | \u001b[0m 0.5521  \u001b[0m | \u001b[0m 0.3486  \u001b[0m | \u001b[0m 1.116   \u001b[0m |\n"],"name":"stdout"}]}]}